; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --function-signature --scrub-attributes --force-update
; DO NOT EDIT -- This file was generated from test/CodeGen/CHERI-Generic/Inputs/stack-bounds-pass-phi.ll
; REQUIRES: asserts
; RUN: opt -mtriple=aarch64 --relocation-model=pic -target-abi purecap -mattr=+morello,+c64 -cheri-bound-allocas %s -o - -S -cheri-stack-bounds=if-needed \
; RUN:    -cheri-stack-bounds-single-intrinsic-threshold=10 -debug-only=cheri-bound-allocas 2>%t.dbg | FileCheck %s
; RUN: llc -mtriple=aarch64 --relocation-model=pic -target-abi purecap -mattr=+morello,+c64 -cheri-stack-bounds=if-needed -O2 -cheri-stack-bounds-single-intrinsic-threshold=10 < %s | %cheri_FileCheck %s -check-prefix ASM
; RUN: FileCheck %s -check-prefix DBG -input-file=%t.dbg
target datalayout = "e-m:e-pf200:128:128:128:64-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-A200-P200-G200"

declare void @foo(i32 addrspace(200)*) addrspace(200)

; Check that we don't attempt to insert stack bounds intrinisics before the PHI at the start of a basic block:
define void @test_phi(i1 %cond) addrspace(200) nounwind {
; ASM-LABEL: test_phi:
; ASM:       .Lfunc_begin0:
; ASM-NEXT:  // %bb.0: // %entry
; ASM-NEXT:    sub csp, csp, #48 // =48
; ASM-NEXT:    stp c30, c19, [csp, #16] // 32-byte Folded Spill
; ASM-NEXT:    tbz w0, #0, .LBB0_2
; ASM-NEXT:  // %bb.1: // %block1
; ASM-NEXT:    mov w8, #1
; ASM-NEXT:    mov w9, #2
; ASM-NEXT:    mov w10, #3
; ASM-NEXT:    mov x0, xzr
; ASM-NEXT:    add c1, csp, #8 // =8
; ASM-NEXT:    b .LBB0_3
; ASM-NEXT:  .LBB0_2: // %block2
; ASM-NEXT:    add c0, csp, #12 // =12
; ASM-NEXT:    mov w8, #4
; ASM-NEXT:    mov w9, #5
; ASM-NEXT:    mov w10, #6
; ASM-NEXT:    add c1, csp, #4 // =4
; ASM-NEXT:    scbnds c0, c0, #4 // =4
; ASM-NEXT:  .LBB0_3: // %phi_block
; ASM-NEXT:    stp w9, w8, [csp, #8]
; ASM-NEXT:    str w10, [csp, #4]
; ASM-NEXT:    scbnds c19, c1, #4 // =4
; ASM-NEXT:    bl foo
; ASM-NEXT:    mov c0, c19
; ASM-NEXT:    bl foo
; ASM-NEXT:    ldp c30, c19, [csp, #16] // 32-byte Folded Reload
; ASM-NEXT:    add csp, csp, #48 // =48
; ASM-NEXT:    ret c30
; CHECK-LABEL: define {{[^@]+}}@test_phi
; CHECK-SAME: (i1 [[COND:%.*]]) addrspace(200) #[[ATTR1:[0-9]+]] {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[ALLOCA1:%.*]] = alloca i32, align 4, addrspace(200)
; CHECK-NEXT:    [[ALLOCA2:%.*]] = alloca i32, align 4, addrspace(200)
; CHECK-NEXT:    [[ALLOCA3:%.*]] = alloca i32, align 4, addrspace(200)
; CHECK-NEXT:    br i1 [[COND]], label [[BLOCK1:%.*]], label [[BLOCK2:%.*]]
; CHECK:       block1:
; CHECK-NEXT:    store i32 1, i32 addrspace(200)* [[ALLOCA1]], align 4
; CHECK-NEXT:    store i32 2, i32 addrspace(200)* [[ALLOCA2]], align 4
; CHECK-NEXT:    store i32 3, i32 addrspace(200)* [[ALLOCA3]], align 4
; CHECK-NEXT:    [[TMP0:%.*]] = bitcast i32 addrspace(200)* [[ALLOCA2]] to i8 addrspace(200)*
; CHECK-NEXT:    [[TMP1:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP0]], i64 4)
; CHECK-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* [[TMP1]] to i32 addrspace(200)*
; CHECK-NEXT:    br label [[PHI_BLOCK:%.*]]
; CHECK:       block2:
; CHECK-NEXT:    store i32 4, i32 addrspace(200)* [[ALLOCA1]], align 4
; CHECK-NEXT:    store i32 5, i32 addrspace(200)* [[ALLOCA2]], align 4
; CHECK-NEXT:    store i32 6, i32 addrspace(200)* [[ALLOCA3]], align 4
; CHECK-NEXT:    [[TMP3:%.*]] = bitcast i32 addrspace(200)* [[ALLOCA1]] to i8 addrspace(200)*
; CHECK-NEXT:    [[TMP4:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP3]], i64 4)
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast i8 addrspace(200)* [[TMP4]] to i32 addrspace(200)*
; CHECK-NEXT:    [[TMP6:%.*]] = bitcast i32 addrspace(200)* [[ALLOCA3]] to i8 addrspace(200)*
; CHECK-NEXT:    [[TMP7:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP6]], i64 4)
; CHECK-NEXT:    [[TMP8:%.*]] = bitcast i8 addrspace(200)* [[TMP7]] to i32 addrspace(200)*
; CHECK-NEXT:    br label [[PHI_BLOCK]]
; CHECK:       phi_block:
; CHECK-NEXT:    [[VAL1:%.*]] = phi i32 addrspace(200)* [ null, [[BLOCK1]] ], [ [[TMP5]], [[BLOCK2]] ]
; CHECK-NEXT:    [[VAL2:%.*]] = phi i32 addrspace(200)* [ [[TMP2]], [[BLOCK1]] ], [ [[TMP8]], [[BLOCK2]] ]
; CHECK-NEXT:    call void @foo(i32 addrspace(200)* [[VAL1]])
; CHECK-NEXT:    call void @foo(i32 addrspace(200)* [[VAL2]])
; CHECK-NEXT:    ret void
;
entry:
  %alloca1 = alloca i32, align 4, addrspace(200)
  %alloca2 = alloca i32, align 4, addrspace(200)
  %alloca3 = alloca i32, align 4, addrspace(200)
  br i1 %cond, label %block1, label %block2

block1:
  store i32 1, i32 addrspace(200)* %alloca1, align 4
  store i32 2, i32 addrspace(200)* %alloca2, align 4
  store i32 3, i32 addrspace(200)* %alloca3, align 4
  br label %phi_block

block2:
  store i32 4, i32 addrspace(200)* %alloca1, align 4
  store i32 5, i32 addrspace(200)* %alloca2, align 4
  store i32 6, i32 addrspace(200)* %alloca3, align 4
  br label %phi_block

phi_block:
  %val1 = phi i32 addrspace(200)* [ null, %block1 ], [ %alloca1, %block2 ]
  %val2 = phi i32 addrspace(200)* [ %alloca2, %block1 ], [ %alloca3, %block2 ]
  call void @foo(i32 addrspace(200)* %val1)
  call void @foo(i32 addrspace(200)* %val2)
  ret void
}

; Check that we don't place all bounded allocas in the entry block, instead only do it in the predecessor
define void @test_only_created_in_predecessor_block(i1 %cond) addrspace(200) nounwind {
; ASM-LABEL: test_only_created_in_predecessor_block:
; ASM:       .Lfunc_begin1:
; ASM-NEXT:  // %bb.0: // %entry
; ASM-NEXT:    sub csp, csp, #32 // =32
; ASM-NEXT:    str c30, [csp, #16] // 16-byte Folded Spill
; ASM-NEXT:    tbz w0, #0, .LBB1_2
; ASM-NEXT:  // %bb.1: // %block1
; ASM-NEXT:    mov w8, #1
; ASM-NEXT:    add c0, csp, #12 // =12
; ASM-NEXT:    str w8, [csp, #12]
; ASM-NEXT:    b .LBB1_3
; ASM-NEXT:  .LBB1_2: // %block2
; ASM-NEXT:    mov w8, #5
; ASM-NEXT:    add c0, csp, #8 // =8
; ASM-NEXT:    str w8, [csp, #8]
; ASM-NEXT:  .LBB1_3: // %phi_block
; ASM-NEXT:    scbnds c0, c0, #4 // =4
; ASM-NEXT:    bl foo
; ASM-NEXT:    ldr c30, [csp, #16] // 16-byte Folded Reload
; ASM-NEXT:    add csp, csp, #32 // =32
; ASM-NEXT:    ret c30
; CHECK-LABEL: define {{[^@]+}}@test_only_created_in_predecessor_block
; CHECK-SAME: (i1 [[COND:%.*]]) addrspace(200) #[[ATTR1]] {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[ALLOCA1:%.*]] = alloca i32, align 4, addrspace(200)
; CHECK-NEXT:    [[ALLOCA2:%.*]] = alloca i32, align 4, addrspace(200)
; CHECK-NEXT:    br i1 [[COND]], label [[BLOCK1:%.*]], label [[BLOCK2:%.*]]
; CHECK:       block1:
; CHECK-NEXT:    store i32 1, i32 addrspace(200)* [[ALLOCA1]], align 4
; CHECK-NEXT:    [[TMP0:%.*]] = bitcast i32 addrspace(200)* [[ALLOCA1]] to i8 addrspace(200)*
; CHECK-NEXT:    [[TMP1:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP0]], i64 4)
; CHECK-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* [[TMP1]] to i32 addrspace(200)*
; CHECK-NEXT:    br label [[PHI_BLOCK:%.*]]
; CHECK:       block2:
; CHECK-NEXT:    store i32 5, i32 addrspace(200)* [[ALLOCA2]], align 4
; CHECK-NEXT:    [[TMP3:%.*]] = bitcast i32 addrspace(200)* [[ALLOCA2]] to i8 addrspace(200)*
; CHECK-NEXT:    [[TMP4:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP3]], i64 4)
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast i8 addrspace(200)* [[TMP4]] to i32 addrspace(200)*
; CHECK-NEXT:    br label [[PHI_BLOCK]]
; CHECK:       phi_block:
; CHECK-NEXT:    [[VAL1:%.*]] = phi i32 addrspace(200)* [ [[TMP2]], [[BLOCK1]] ], [ [[TMP5]], [[BLOCK2]] ]
; CHECK-NEXT:    call void @foo(i32 addrspace(200)* [[VAL1]])
; CHECK-NEXT:    ret void
;
entry:
  %alloca1 = alloca i32, align 4, addrspace(200)
  %alloca2 = alloca i32, align 4, addrspace(200)
  br i1 %cond, label %block1, label %block2

block1:
  store i32 1, i32 addrspace(200)* %alloca1, align 4
  br label %phi_block

block2:
  store i32 5, i32 addrspace(200)* %alloca2, align 4
  br label %phi_block

phi_block:
  %val1 = phi i32 addrspace(200)* [ %alloca1, %block1 ], [ %alloca2, %block2 ]
  call void @foo(i32 addrspace(200)* %val1)
  ret void
}

; DBG: -Adding stack bounds since phi user needs bounds:   call void @foo(i32 addrspace(200)* %val1)
; DBG: test_phi: 1 of 3 users need bounds for   %alloca1 = alloca i32, align 4, addrspace(200)
; DBG: -Adding stack bounds since phi user needs bounds:   call void @foo(i32 addrspace(200)* %val2)
; DBG: test_phi: 1 of 3 users need bounds for   %alloca2 = alloca i32, align 4, addrspace(200)
; DBG: -Adding stack bounds since phi user needs bounds:   call void @foo(i32 addrspace(200)* %val2)
; DBG: test_phi: 1 of 3 users need bounds for   %alloca3 = alloca i32, align 4, addrspace(200)
