; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --function-signature --scrub-attributes --force-update
; DO NOT EDIT -- This file was generated from test/CodeGen/CHERI-Generic/Inputs/stack-spill-unnecessary.c.ll
; The new CheriBoundedStackPseudo instruction lets us pretend that the incoffset+csetbounds
; is a single trivially rematerizable instruction so it can freely move it around to avoid stack spills.
; Previously we were moving the allocation of the register that is only used later to the beginning of
; the function and saving+restoring it instead of materializing it just before

; RUN: llc -mtriple=aarch64 --relocation-model=pic -target-abi purecap -mattr=+morello,+c64 -O2 --cheri-stack-bounds-single-intrinsic-threshold=0 < %s | %cheri_FileCheck %s --check-prefixes=CHECK
; Always use a single intrinsic for the calls (should result in same codegen)
; RUN: llc -mtriple=aarch64 --relocation-model=pic -target-abi purecap -mattr=+morello,+c64 -O2 --cheri-stack-bounds-single-intrinsic-threshold=0 < %s | %cheri_FileCheck %s --check-prefixes=CHECK
; RUN: sed 's/addrspace(200)/addrspace(0)/g' %s | llc -mtriple=aarch64 --relocation-model=pic -target-abi aapcs -mattr=+morello,-c64 | FileCheck --check-prefix HYBRID %s


declare void @foo() addrspace(200)
declare void @one_arg(i32 addrspace(200)*) addrspace(200)
declare void @multi_arg(i32 addrspace(200)* %start, i32 addrspace(200)* %end, i8 addrspace(200)* %buf) addrspace(200)

define void @use_after_call() addrspace(200) nounwind {
; CHECK-LABEL: use_after_call:
; CHECK:       .Lfunc_begin0:
; CHECK-NEXT:  // %bb.0:
; CHECK-NEXT:    sub csp, csp, #48
; CHECK-NEXT:    mov w8, #123
; CHECK-NEXT:    add c0, csp, #12
; CHECK-NEXT:    stp c30, c19, [csp, #16] // 32-byte Folded Spill
; CHECK-NEXT:    scbnds c19, c0, #4 // =4
; CHECK-NEXT:    str w8, [csp, #12]
; CHECK-NEXT:    bl foo
; CHECK-NEXT:    mov c0, c19
; CHECK-NEXT:    bl one_arg
; CHECK-NEXT:    ldp c30, c19, [csp, #16] // 32-byte Folded Reload
; CHECK-NEXT:    add csp, csp, #48
; CHECK-NEXT:    ret c30
;
; HYBRID-LABEL: use_after_call:
; HYBRID:       // %bb.0:
; HYBRID-NEXT:    str x30, [sp, #-16]! // 8-byte Folded Spill
; HYBRID-NEXT:    mov w8, #123
; HYBRID-NEXT:    str w8, [sp, #12]
; HYBRID-NEXT:    bl foo
; HYBRID-NEXT:    add x0, sp, #12
; HYBRID-NEXT:    bl one_arg
; HYBRID-NEXT:    ldr x30, [sp], #16 // 8-byte Folded Reload
; HYBRID-NEXT:    ret
  %x = alloca i32, align 4, addrspace(200)
  store i32 123, i32 addrspace(200)* %x, align 4
  call void @foo()
  call void @one_arg(i32 addrspace(200)* %x)
  ret void
}

define void @use_after_call_no_store() addrspace(200) nounwind {
; CHECK-LABEL: use_after_call_no_store:
; CHECK:       .Lfunc_begin1:
; CHECK-NEXT:  // %bb.0:
; CHECK-NEXT:    sub csp, csp, #64
; CHECK-NEXT:    add c0, csp, #12
; CHECK-NEXT:    add c1, csp, #8
; CHECK-NEXT:    str c30, [csp, #16] // 16-byte Folded Spill
; CHECK-NEXT:    stp c20, c19, [csp, #32] // 32-byte Folded Spill
; CHECK-NEXT:    scbnds c19, c0, #4 // =4
; CHECK-NEXT:    scbnds c20, c1, #4 // =4
; CHECK-NEXT:    bl foo
; CHECK-NEXT:    mov c0, c19
; CHECK-NEXT:    bl one_arg
; CHECK-NEXT:    mov c0, c20
; CHECK-NEXT:    bl one_arg
; CHECK-NEXT:    ldp c20, c19, [csp, #32] // 32-byte Folded Reload
; CHECK-NEXT:    ldr c30, [csp, #16] // 16-byte Folded Reload
; CHECK-NEXT:    add csp, csp, #64
; CHECK-NEXT:    ret c30
;
; HYBRID-LABEL: use_after_call_no_store:
; HYBRID:       // %bb.0:
; HYBRID-NEXT:    str x30, [sp, #-16]! // 8-byte Folded Spill
; HYBRID-NEXT:    bl foo
; HYBRID-NEXT:    add x0, sp, #12
; HYBRID-NEXT:    bl one_arg
; HYBRID-NEXT:    add x0, sp, #8
; HYBRID-NEXT:    bl one_arg
; HYBRID-NEXT:    ldr x30, [sp], #16 // 8-byte Folded Reload
; HYBRID-NEXT:    ret
  %x = alloca i32, align 4, addrspace(200)
  %y = alloca i32, align 4, addrspace(200)
  call void @foo()
  call void @one_arg(i32 addrspace(200)* %x)
  call void @one_arg(i32 addrspace(200)* %y)
  ret void
}

define void @multi_use() addrspace(200) nounwind {
; CHECK-LABEL: multi_use:
; CHECK:       .Lfunc_begin2:
; CHECK-NEXT:  // %bb.0:
; CHECK-NEXT:    sub csp, csp, #64
; CHECK-NEXT:    add c0, csp, #12
; CHECK-NEXT:    add c1, csp, #8
; CHECK-NEXT:    str c30, [csp, #16] // 16-byte Folded Spill
; CHECK-NEXT:    stp c20, c19, [csp, #32] // 32-byte Folded Spill
; CHECK-NEXT:    scbnds c19, c0, #4 // =4
; CHECK-NEXT:    scbnds c20, c1, #4 // =4
; CHECK-NEXT:    bl foo
; CHECK-NEXT:    add c1, c20, #4
; CHECK-NEXT:    add c2, c20, #1
; CHECK-NEXT:    mov c0, c20
; CHECK-NEXT:    bl multi_arg
; CHECK-NEXT:    mov c0, c19
; CHECK-NEXT:    bl one_arg
; CHECK-NEXT:    mov c0, c20
; CHECK-NEXT:    bl one_arg
; CHECK-NEXT:    ldp c20, c19, [csp, #32] // 32-byte Folded Reload
; CHECK-NEXT:    ldr c30, [csp, #16] // 16-byte Folded Reload
; CHECK-NEXT:    add csp, csp, #64
; CHECK-NEXT:    ret c30
;
; HYBRID-LABEL: multi_use:
; HYBRID:       // %bb.0:
; HYBRID-NEXT:    str x30, [sp, #-16]! // 8-byte Folded Spill
; HYBRID-NEXT:    bl foo
; HYBRID-NEXT:    add x8, sp, #8
; HYBRID-NEXT:    add x0, sp, #8
; HYBRID-NEXT:    add x1, x8, #4
; HYBRID-NEXT:    orr x2, x8, #0x1
; HYBRID-NEXT:    bl multi_arg
; HYBRID-NEXT:    add x0, sp, #12
; HYBRID-NEXT:    bl one_arg
; HYBRID-NEXT:    add x0, sp, #8
; HYBRID-NEXT:    bl one_arg
; HYBRID-NEXT:    ldr x30, [sp], #16 // 8-byte Folded Reload
; HYBRID-NEXT:    ret
  %y = alloca i32, align 4, addrspace(200)
  %x = alloca i32, align 4, addrspace(200)
  call void @foo()
  %x_plus0 = getelementptr inbounds i32, i32 addrspace(200)* %x, i32 0
  %x_plus1 = getelementptr i32, i32 addrspace(200)* %x, i32 1
  %x_i8 = bitcast i32 addrspace(200)* %x to i8 addrspace(200)*
  %x_i8_plus_1 = getelementptr inbounds i8, i8 addrspace(200)* %x_i8, i32 1
  call void @multi_arg(i32 addrspace(200)* %x_plus0, i32 addrspace(200)* %x_plus1, i8 addrspace(200)* %x_i8_plus_1)
  call void @one_arg(i32 addrspace(200)* %y)
  call void @one_arg(i32 addrspace(200)* %x)
  ret void
}
