; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --function-signature --scrub-attributes --force-update
; DO NOT EDIT -- This file was generated from test/CodeGen/CHERI-Generic/Inputs/stack-bounds-dynamic-alloca.ll
; RUN: opt -mtriple=aarch64 --relocation-model=pic -target-abi purecap -mattr=+morello,+c64 -cheri-bound-allocas -o - -S %s | FileCheck %s
; RUN: llc -mtriple=aarch64 --relocation-model=pic -target-abi purecap -mattr=+morello,+c64 -O0 %s -o - | FileCheck %s -check-prefix ASM
; RUN: llc -mtriple=aarch64 --relocation-model=pic -target-abi purecap -mattr=+morello,+c64 -O2 %s -o - | FileCheck %s -check-prefix ASM-OPT

; reduced C test case:
; __builtin_va_list a;
; char *b;
; void c() {
;   while (__builtin_va_arg(a, char))
;     b = __builtin_alloca(sizeof(b));
;   d(b);
; }
target datalayout = "e-m:e-pf200:128:128:128:64-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-A200-P200-G200"

declare i32 @use_alloca(i8 addrspace(200)*) local_unnamed_addr addrspace(200)

define i32 @alloca_in_entry(i1 %arg) local_unnamed_addr addrspace(200) nounwind {
; ASM-LABEL: alloca_in_entry:
; ASM:       .Lfunc_begin0:
; ASM-NEXT:  // %bb.0: // %entry
; ASM-NEXT:    sub csp, csp, #32
; ASM-NEXT:    str c30, [csp, #16] // 16-byte Folded Spill
; ASM-NEXT:    // kill: def $w1 killed $w0
; ASM-NEXT:    tbz w0, #0, .LBB0_4
; ASM-NEXT:    b .LBB0_1
; ASM-NEXT:  .LBB0_1: // %do_alloca
; ASM-NEXT:    b .LBB0_2
; ASM-NEXT:  .LBB0_2: // %use_alloca_no_bounds
; ASM-NEXT:    mov w8, #1234
; ASM-NEXT:    // kill: def $x8 killed $w8
; ASM-NEXT:    str x8, [csp, #8]
; ASM-NEXT:    b .LBB0_3
; ASM-NEXT:  .LBB0_3: // %use_alloca_need_bounds
; ASM-NEXT:    mov c0, csp
; ASM-NEXT:    scbnds c0, c0, #16 // =16
; ASM-NEXT:    bl use_alloca
; ASM-NEXT:    b .LBB0_4
; ASM-NEXT:  .LBB0_4: // %exit
; ASM-NEXT:    mov w0, #123
; ASM-NEXT:    ldr c30, [csp, #16] // 16-byte Folded Reload
; ASM-NEXT:    add csp, csp, #32
; ASM-NEXT:    ret c30
;
; ASM-OPT-LABEL: alloca_in_entry:
; ASM-OPT:       .Lfunc_begin0:
; ASM-OPT-NEXT:  // %bb.0: // %entry
; ASM-OPT-NEXT:    tbz w0, #0, .LBB0_2
; ASM-OPT-NEXT:  // %bb.1: // %use_alloca_no_bounds
; ASM-OPT-NEXT:    sub csp, csp, #32
; ASM-OPT-NEXT:    mov w8, #1234
; ASM-OPT-NEXT:    mov c0, csp
; ASM-OPT-NEXT:    scbnds c0, c0, #16 // =16
; ASM-OPT-NEXT:    str c30, [csp, #16] // 16-byte Folded Spill
; ASM-OPT-NEXT:    str x8, [csp, #8]
; ASM-OPT-NEXT:    bl use_alloca
; ASM-OPT-NEXT:    ldr c30, [csp, #16] // 16-byte Folded Reload
; ASM-OPT-NEXT:    add csp, csp, #32
; ASM-OPT-NEXT:  .LBB0_2: // %exit
; ASM-OPT-NEXT:    mov w0, #123
; ASM-OPT-NEXT:    ret c30
; CHECK-LABEL: define {{[^@]+}}@alloca_in_entry
; CHECK-SAME: (i1 [[ARG:%.*]]) local_unnamed_addr addrspace(200) #[[ATTR1:[0-9]+]] {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[ALLOCA:%.*]] = alloca [16 x i8], align 16, addrspace(200)
; CHECK-NEXT:    br i1 [[ARG]], label [[DO_ALLOCA:%.*]], label [[EXIT:%.*]]
; CHECK:       do_alloca:
; CHECK-NEXT:    br label [[USE_ALLOCA_NO_BOUNDS:%.*]]
; CHECK:       use_alloca_no_bounds:
; CHECK-NEXT:    [[PTR:%.*]] = bitcast [16 x i8] addrspace(200)* [[ALLOCA]] to i64 addrspace(200)*
; CHECK-NEXT:    [[PTR_PLUS_ONE:%.*]] = getelementptr i64, i64 addrspace(200)* [[PTR]], i64 1
; CHECK-NEXT:    store i64 1234, i64 addrspace(200)* [[PTR_PLUS_ONE]], align 8
; CHECK-NEXT:    br label [[USE_ALLOCA_NEED_BOUNDS:%.*]]
; CHECK:       use_alloca_need_bounds:
; CHECK-NEXT:    [[TMP0:%.*]] = bitcast [16 x i8] addrspace(200)* [[ALLOCA]] to i8 addrspace(200)*
; CHECK-NEXT:    [[TMP1:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP0]], i64 16)
; CHECK-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* [[TMP1]] to [16 x i8] addrspace(200)*
; CHECK-NEXT:    [[DOTSUB_LE:%.*]] = getelementptr inbounds [16 x i8], [16 x i8] addrspace(200)* [[TMP2]], i64 0, i64 0
; CHECK-NEXT:    [[CALL:%.*]] = call signext i32 @use_alloca(i8 addrspace(200)* [[DOTSUB_LE]])
; CHECK-NEXT:    br label [[EXIT]]
; CHECK:       exit:
; CHECK-NEXT:    ret i32 123
;
entry:
  %alloca = alloca [16 x i8], align 16, addrspace(200)
  br i1 %arg, label %do_alloca, label %exit

do_alloca:
  br label %use_alloca_no_bounds

use_alloca_no_bounds:
  %ptr = bitcast [16 x i8] addrspace(200)* %alloca to i64 addrspace(200)*
  %ptr_plus_one = getelementptr i64, i64 addrspace(200)* %ptr, i64 1
  store i64 1234, i64 addrspace(200)* %ptr_plus_one, align 8
  br label %use_alloca_need_bounds

use_alloca_need_bounds:
  %.sub.le = getelementptr inbounds [16 x i8], [16 x i8] addrspace(200)* %alloca, i64 0, i64 0
  %call = call signext i32 @use_alloca(i8 addrspace(200)* %.sub.le)
  br label %exit

exit:
  ret i32 123
}

define i32 @alloca_not_in_entry(i1 %arg) local_unnamed_addr addrspace(200) nounwind {
; ASM-LABEL: alloca_not_in_entry:
; ASM:       .Lfunc_begin1:
; ASM-NEXT:  // %bb.0: // %entry
; ASM-NEXT:    stp c29, c30, [csp, #-32]! // 32-byte Folded Spill
; ASM-NEXT:    mov c29, csp
; ASM-NEXT:    sub csp, csp, #32
; ASM-NEXT:    // kill: def $w1 killed $w0
; ASM-NEXT:    tbz w0, #0, .LBB1_4
; ASM-NEXT:    b .LBB1_1
; ASM-NEXT:  .LBB1_1: // %do_alloca
; ASM-NEXT:    mov w8, #16
; ASM-NEXT:    mov w10, w8
; ASM-NEXT:    rrlen x8, x10
; ASM-NEXT:    mov c0, csp
; ASM-NEXT:    gcvalue x9, c0
; ASM-NEXT:    subs x9, x9, x8
; ASM-NEXT:    rrmask x10, x10
; ASM-NEXT:    and x9, x9, x10
; ASM-NEXT:    scvalue c1, c0, x9
; ASM-NEXT:    scbnds c0, c1, x8
; ASM-NEXT:    mov csp, c1
; ASM-NEXT:    mov c1, c0
; ASM-NEXT:    stur c1, [c29, #-32] // 16-byte Folded Spill
; ASM-NEXT:    scbnds c0, c0, #16 // =16
; ASM-NEXT:    stur c0, [c29, #-16] // 16-byte Folded Spill
; ASM-NEXT:    b .LBB1_2
; ASM-NEXT:  .LBB1_2: // %use_alloca_no_bounds
; ASM-NEXT:    ldur c0, [c29, #-32] // 16-byte Folded Reload
; ASM-NEXT:    mov w8, #1234
; ASM-NEXT:    // kill: def $x8 killed $w8
; ASM-NEXT:    str x8, [c0, #8]
; ASM-NEXT:    b .LBB1_3
; ASM-NEXT:  .LBB1_3: // %use_alloca_need_bounds
; ASM-NEXT:    ldur c0, [c29, #-16] // 16-byte Folded Reload
; ASM-NEXT:    bl use_alloca
; ASM-NEXT:    b .LBB1_4
; ASM-NEXT:  .LBB1_4: // %exit
; ASM-NEXT:    mov w0, #123
; ASM-NEXT:    mov csp, c29
; ASM-NEXT:    ldp c29, c30, [csp], #32 // 32-byte Folded Reload
; ASM-NEXT:    ret c30
;
; ASM-OPT-LABEL: alloca_not_in_entry:
; ASM-OPT:       .Lfunc_begin1:
; ASM-OPT-NEXT:  // %bb.0: // %entry
; ASM-OPT-NEXT:    tbz w0, #0, .LBB1_2
; ASM-OPT-NEXT:  // %bb.1: // %do_alloca
; ASM-OPT-NEXT:    stp c29, c30, [csp, #-32]! // 32-byte Folded Spill
; ASM-OPT-NEXT:    mov w9, #16
; ASM-OPT-NEXT:    mov x8, sp
; ASM-OPT-NEXT:    mov c29, csp
; ASM-OPT-NEXT:    rrlen x10, x9
; ASM-OPT-NEXT:    rrmask x9, x9
; ASM-OPT-NEXT:    sub x8, x8, x10
; ASM-OPT-NEXT:    and x8, x8, x9
; ASM-OPT-NEXT:    scvalue c0, csp, x8
; ASM-OPT-NEXT:    scbnds c1, c0, x10
; ASM-OPT-NEXT:    mov csp, c0
; ASM-OPT-NEXT:    mov w8, #1234
; ASM-OPT-NEXT:    scbnds c0, c1, #16 // =16
; ASM-OPT-NEXT:    str x8, [c1, #8]
; ASM-OPT-NEXT:    bl use_alloca
; ASM-OPT-NEXT:    mov csp, c29
; ASM-OPT-NEXT:    ldp c29, c30, [csp], #32 // 32-byte Folded Reload
; ASM-OPT-NEXT:  .LBB1_2: // %exit
; ASM-OPT-NEXT:    mov w0, #123
; ASM-OPT-NEXT:    ret c30
; CHECK-LABEL: define {{[^@]+}}@alloca_not_in_entry
; CHECK-SAME: (i1 [[ARG:%.*]]) local_unnamed_addr addrspace(200) #[[ATTR1]] {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    br i1 [[ARG]], label [[DO_ALLOCA:%.*]], label [[EXIT:%.*]]
; CHECK:       do_alloca:
; CHECK-NEXT:    [[ALLOCA:%.*]] = alloca [16 x i8], align 16, addrspace(200)
; CHECK-NEXT:    [[TMP0:%.*]] = bitcast [16 x i8] addrspace(200)* [[ALLOCA]] to i8 addrspace(200)*
; CHECK-NEXT:    [[TMP1:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.dynamic.i64(i8 addrspace(200)* [[TMP0]], i64 16)
; CHECK-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* [[TMP1]] to [16 x i8] addrspace(200)*
; CHECK-NEXT:    br label [[USE_ALLOCA_NO_BOUNDS:%.*]]
; CHECK:       use_alloca_no_bounds:
; CHECK-NEXT:    [[PTR:%.*]] = bitcast [16 x i8] addrspace(200)* [[ALLOCA]] to i64 addrspace(200)*
; CHECK-NEXT:    [[PTR_PLUS_ONE:%.*]] = getelementptr i64, i64 addrspace(200)* [[PTR]], i64 1
; CHECK-NEXT:    store i64 1234, i64 addrspace(200)* [[PTR_PLUS_ONE]], align 8
; CHECK-NEXT:    br label [[USE_ALLOCA_NEED_BOUNDS:%.*]]
; CHECK:       use_alloca_need_bounds:
; CHECK-NEXT:    [[DOTSUB_LE:%.*]] = getelementptr inbounds [16 x i8], [16 x i8] addrspace(200)* [[TMP2]], i64 0, i64 0
; CHECK-NEXT:    [[CALL:%.*]] = call signext i32 @use_alloca(i8 addrspace(200)* [[DOTSUB_LE]])
; CHECK-NEXT:    br label [[EXIT]]
; CHECK:       exit:
; CHECK-NEXT:    ret i32 123
;
entry:
  br i1 %arg, label %do_alloca, label %exit

do_alloca:
  %alloca = alloca [16 x i8], align 16, addrspace(200)
  br label %use_alloca_no_bounds

use_alloca_no_bounds:
  %ptr = bitcast [16 x i8] addrspace(200)* %alloca to i64 addrspace(200)*
  %ptr_plus_one = getelementptr i64, i64 addrspace(200)* %ptr, i64 1
  store i64 1234, i64 addrspace(200)* %ptr_plus_one, align 8
  br label %use_alloca_need_bounds

use_alloca_need_bounds:
  %.sub.le = getelementptr inbounds [16 x i8], [16 x i8] addrspace(200)* %alloca, i64 0, i64 0
  %call = call signext i32 @use_alloca(i8 addrspace(200)* %.sub.le)
  br label %exit

exit:
  ret i32 123
}

; The original reduced test case from libc/gen/exec.c
; We can't use llvm.cheri.bounded.stack.cap.i64 here, since that only works for static allocas:
define i32 @crash_reproducer(i1 %arg) local_unnamed_addr addrspace(200) nounwind {
; ASM-LABEL: crash_reproducer:
; ASM:       .Lfunc_begin2:
; ASM-NEXT:  // %bb.0: // %entry
; ASM-NEXT:    stp c29, c30, [csp, #-32]! // 32-byte Folded Spill
; ASM-NEXT:    mov c29, csp
; ASM-NEXT:    sub csp, csp, #32
; ASM-NEXT:    // kill: def $w1 killed $w0
; ASM-NEXT:    tbz w0, #0, .LBB2_2
; ASM-NEXT:    b .LBB2_1
; ASM-NEXT:  .LBB2_1: // %entry.while.end_crit_edge
; ASM-NEXT:  .LBB2_2: // %while.body
; ASM-NEXT:    mov w8, #16
; ASM-NEXT:    mov w10, w8
; ASM-NEXT:    rrlen x8, x10
; ASM-NEXT:    mov c0, csp
; ASM-NEXT:    gcvalue x9, c0
; ASM-NEXT:    subs x9, x9, x8
; ASM-NEXT:    rrmask x10, x10
; ASM-NEXT:    and x9, x9, x10
; ASM-NEXT:    scvalue c1, c0, x9
; ASM-NEXT:    scbnds c0, c1, x8
; ASM-NEXT:    mov csp, c1
; ASM-NEXT:    scbnds c0, c0, #16 // =16
; ASM-NEXT:    stur c0, [c29, #-16] // 16-byte Folded Spill
; ASM-NEXT:    b .LBB2_3
; ASM-NEXT:  .LBB2_3: // %while.end.loopexit
; ASM-NEXT:    ldur c0, [c29, #-16] // 16-byte Folded Reload
; ASM-NEXT:    stur c0, [c29, #-32] // 16-byte Folded Spill
; ASM-NEXT:    b .LBB2_4
; ASM-NEXT:  .LBB2_4: // %while.end
; ASM-NEXT:    ldur c0, [c29, #-32] // 16-byte Folded Reload
; ASM-NEXT:    bl use_alloca
; ASM-NEXT:    add w0, w0, #1234
; ASM-NEXT:    mov csp, c29
; ASM-NEXT:    ldp c29, c30, [csp], #32 // 32-byte Folded Reload
; ASM-NEXT:    ret c30
;
; ASM-OPT-LABEL: crash_reproducer:
; ASM-OPT:       .Lfunc_begin2:
; ASM-OPT-NEXT:  // %bb.0: // %entry
; ASM-OPT-NEXT:    sub csp, csp, #32
; ASM-OPT-NEXT:    mov c0, csp
; ASM-OPT-NEXT:    str c30, [csp, #16] // 16-byte Folded Spill
; ASM-OPT-NEXT:    scbnds c0, c0, #16 // =16
; ASM-OPT-NEXT:    bl use_alloca
; ASM-OPT-NEXT:    ldr c30, [csp, #16] // 16-byte Folded Reload
; ASM-OPT-NEXT:    add w0, w0, #1234
; ASM-OPT-NEXT:    add csp, csp, #32
; ASM-OPT-NEXT:    ret c30
; CHECK-LABEL: define {{[^@]+}}@crash_reproducer
; CHECK-SAME: (i1 [[ARG:%.*]]) local_unnamed_addr addrspace(200) #[[ATTR1]] {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    br i1 [[ARG]], label [[ENTRY_WHILE_END_CRIT_EDGE:%.*]], label [[WHILE_BODY:%.*]]
; CHECK:       entry.while.end_crit_edge:
; CHECK-NEXT:    unreachable
; CHECK:       while.body:
; CHECK-NEXT:    [[TMP0:%.*]] = alloca [16 x i8], align 16, addrspace(200)
; CHECK-NEXT:    [[TMP1:%.*]] = bitcast [16 x i8] addrspace(200)* [[TMP0]] to i8 addrspace(200)*
; CHECK-NEXT:    [[TMP2:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.dynamic.i64(i8 addrspace(200)* [[TMP1]], i64 16)
; CHECK-NEXT:    [[TMP3:%.*]] = bitcast i8 addrspace(200)* [[TMP2]] to [16 x i8] addrspace(200)*
; CHECK-NEXT:    br label [[WHILE_END_LOOPEXIT:%.*]]
; CHECK:       while.end.loopexit:
; CHECK-NEXT:    [[DOTSUB_LE:%.*]] = getelementptr inbounds [16 x i8], [16 x i8] addrspace(200)* [[TMP3]], i64 0, i64 0
; CHECK-NEXT:    br label [[WHILE_END:%.*]]
; CHECK:       while.end:
; CHECK-NEXT:    [[CALL:%.*]] = call signext i32 @use_alloca(i8 addrspace(200)* [[DOTSUB_LE]])
; CHECK-NEXT:    [[RESULT:%.*]] = add i32 [[CALL]], 1234
; CHECK-NEXT:    ret i32 [[RESULT]]
;
entry:
  br i1 %arg, label %entry.while.end_crit_edge, label %while.body

entry.while.end_crit_edge:
  unreachable

while.body:
  %0 = alloca [16 x i8], align 16, addrspace(200)
  br label %while.end.loopexit

while.end.loopexit:
  %.sub.le = getelementptr inbounds [16 x i8], [16 x i8] addrspace(200)* %0, i64 0, i64 0
  br label %while.end

while.end:
  %call = call signext i32 @use_alloca(i8 addrspace(200)* %.sub.le)
  %result = add i32 %call, 1234
  ret i32 %result
}
