// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %cheri_purecap_cc1 -o - -O0 -emit-llvm %s | %cheri_FileCheck %s
// RUN: %cheri_cc1 -o - -O0 -emit-llvm %s | %cheri_FileCheck %s -check-prefix=HYBRID
// https://github.com/CTSRD-CHERI/clang/issues/178
struct a {
  void *__capability ptr;
};
// CHECK-LABEL: @c(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[FIRST:%.*]] = alloca [[STRUCT_A:%.*]], align 16, addrspace(200)
// CHECK-NEXT:    [[SECOND:%.*]] = alloca [[STRUCT_A]], align 16, addrspace(200)
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast [[STRUCT_A]] addrspace(200)* [[SECOND]] to i8 addrspace(200)*
// CHECK-NEXT:    [[PTR:%.*]] = getelementptr inbounds [[STRUCT_A]], [[STRUCT_A]] addrspace(200)* [[FIRST]], i32 0, i32 0
// CHECK-NEXT:    store i8 addrspace(200)* [[TMP0]], i8 addrspace(200)* addrspace(200)* [[PTR]], align 16
// CHECK-NEXT:    ret void
//
// HYBRID-LABEL: @c(
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[FIRST:%.*]] = alloca [[STRUCT_A:%.*]], align 16
// HYBRID-NEXT:    [[SECOND:%.*]] = alloca [[STRUCT_A]], align 16
// HYBRID-NEXT:    [[TMP0:%.*]] = bitcast %struct.a* [[SECOND]] to i8*
// HYBRID-NEXT:    [[TMP1:%.*]] = addrspacecast i8* [[TMP0]] to i8 addrspace(200)*
// HYBRID-NEXT:    [[PTR:%.*]] = getelementptr inbounds [[STRUCT_A]], %struct.a* [[FIRST]], i32 0, i32 0
// HYBRID-NEXT:    store i8 addrspace(200)* [[TMP1]], i8 addrspace(200)** [[PTR]], align 16
// HYBRID-NEXT:    ret void
//
void c(void) {
  struct a first;
  struct a second;
  first.ptr = (__cheri_tocap void *__capability) & second;
}

struct foo;

// CHECK-LABEL: @test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[FOOPTR_ADDR:%.*]] = alloca [[STRUCT_FOO:%.*]] addrspace(200)*, align 16, addrspace(200)
// CHECK-NEXT:    [[FOOCAP_ADDR:%.*]] = alloca [[STRUCT_FOO]] addrspace(200)*, align 16, addrspace(200)
// CHECK-NEXT:    [[PTR:%.*]] = alloca i32 addrspace(200)*, align 16, addrspace(200)
// CHECK-NEXT:    [[CAP:%.*]] = alloca i32 addrspace(200)*, align 16, addrspace(200)
// CHECK-NEXT:    store [[STRUCT_FOO]] addrspace(200)* [[FOOPTR:%.*]], [[STRUCT_FOO]] addrspace(200)* addrspace(200)* [[FOOPTR_ADDR]], align 16
// CHECK-NEXT:    store [[STRUCT_FOO]] addrspace(200)* [[FOOCAP:%.*]], [[STRUCT_FOO]] addrspace(200)* addrspace(200)* [[FOOCAP_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load [[STRUCT_FOO]] addrspace(200)*, [[STRUCT_FOO]] addrspace(200)* addrspace(200)* [[FOOCAP_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast [[STRUCT_FOO]] addrspace(200)* [[TMP0]] to i8 addrspace(200)*
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* [[TMP1]] to i32 addrspace(200)*
// CHECK-NEXT:    store i32 addrspace(200)* [[TMP2]], i32 addrspace(200)* addrspace(200)* [[PTR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load [[STRUCT_FOO]] addrspace(200)*, [[STRUCT_FOO]] addrspace(200)* addrspace(200)* [[FOOPTR_ADDR]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast [[STRUCT_FOO]] addrspace(200)* [[TMP3]] to i8 addrspace(200)*
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i8 addrspace(200)* [[TMP4]] to i32 addrspace(200)*
// CHECK-NEXT:    store i32 addrspace(200)* [[TMP5]], i32 addrspace(200)* addrspace(200)* [[CAP]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = load i32 addrspace(200)*, i32 addrspace(200)* addrspace(200)* [[PTR]], align 16
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, i32 addrspace(200)* [[TMP6]], align 4
// CHECK-NEXT:    [[TMP8:%.*]] = load i32 addrspace(200)*, i32 addrspace(200)* addrspace(200)* [[CAP]], align 16
// CHECK-NEXT:    [[TMP9:%.*]] = load i32, i32 addrspace(200)* [[TMP8]], align 4
// CHECK-NEXT:    [[ADD:%.*]] = add nsw i32 [[TMP7]], [[TMP9]]
// CHECK-NEXT:    ret i32 [[ADD]]
//
// HYBRID-LABEL: @test(
// HYBRID-NEXT:  entry:
// HYBRID-NEXT:    [[FOOPTR_ADDR:%.*]] = alloca %struct.foo*, align 8
// HYBRID-NEXT:    [[FOOCAP_ADDR:%.*]] = alloca [[STRUCT_FOO:%.*]] addrspace(200)*, align 16
// HYBRID-NEXT:    [[PTR:%.*]] = alloca i32*, align 8
// HYBRID-NEXT:    [[CAP:%.*]] = alloca i32 addrspace(200)*, align 16
// HYBRID-NEXT:    store %struct.foo* [[FOOPTR:%.*]], %struct.foo** [[FOOPTR_ADDR]], align 8
// HYBRID-NEXT:    store [[STRUCT_FOO]] addrspace(200)* [[FOOCAP:%.*]], [[STRUCT_FOO]] addrspace(200)** [[FOOCAP_ADDR]], align 16
// HYBRID-NEXT:    [[TMP0:%.*]] = load [[STRUCT_FOO]] addrspace(200)*, [[STRUCT_FOO]] addrspace(200)** [[FOOCAP_ADDR]], align 16
// HYBRID-NEXT:    [[TMP1:%.*]] = bitcast [[STRUCT_FOO]] addrspace(200)* [[TMP0]] to i8 addrspace(200)*
// HYBRID-NEXT:    [[TMP2:%.*]] = addrspacecast i8 addrspace(200)* [[TMP1]] to i8*
// HYBRID-NEXT:    [[TMP3:%.*]] = bitcast i8* [[TMP2]] to i32*
// HYBRID-NEXT:    store i32* [[TMP3]], i32** [[PTR]], align 8
// HYBRID-NEXT:    [[TMP4:%.*]] = load %struct.foo*, %struct.foo** [[FOOPTR_ADDR]], align 8
// HYBRID-NEXT:    [[TMP5:%.*]] = bitcast %struct.foo* [[TMP4]] to i8*
// HYBRID-NEXT:    [[TMP6:%.*]] = addrspacecast i8* [[TMP5]] to i8 addrspace(200)*
// HYBRID-NEXT:    [[TMP7:%.*]] = bitcast i8 addrspace(200)* [[TMP6]] to i32 addrspace(200)*
// HYBRID-NEXT:    store i32 addrspace(200)* [[TMP7]], i32 addrspace(200)** [[CAP]], align 16
// HYBRID-NEXT:    [[TMP8:%.*]] = load i32*, i32** [[PTR]], align 8
// HYBRID-NEXT:    [[TMP9:%.*]] = load i32, i32* [[TMP8]], align 4
// HYBRID-NEXT:    [[TMP10:%.*]] = load i32 addrspace(200)*, i32 addrspace(200)** [[CAP]], align 16
// HYBRID-NEXT:    [[TMP11:%.*]] = load i32, i32 addrspace(200)* [[TMP10]], align 4
// HYBRID-NEXT:    [[ADD:%.*]] = add nsw i32 [[TMP9]], [[TMP11]]
// HYBRID-NEXT:    ret i32 [[ADD]]
//
int test(struct foo* fooptr, struct foo* __capability foocap) {
  int* ptr = (__cheri_fromcap void*)foocap;
  int* __capability cap = (__cheri_tocap void* __capability)fooptr;
  return *ptr + *cap;
}
